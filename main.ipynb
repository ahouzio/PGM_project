{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import argparse\n","from train import train_sn\n","from test import test_ncsn, inpaint_ncsn, test_mix\n","from load_data import load_dataset\n","import torch\n","from models import *\n","import logging\n","import torch.distributions as TD\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from utils import plot_score_function, distribution2score\n","import os\n","import pandas as pd\n","import seaborn as sns\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from models import NoiseConditionalScoreNetwork\n","import torch\n","import os\n","from load_data import load_dataset\n","import gc\n","from tqdm import tqdm\n","from torchvision.utils import save_image, make_grid\n","from PIL import Image\n","from utils import distribution2score\n","import matplotlib.pyplot as plt\n","import torch\n","from tqdm import tqdm\n","import numpy as np\n","from utils import batch_jacobian\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["logging.basicConfig(level=logging.INFO)\n","logg = logging.getLogger(__name__)"]},{"cell_type":"markdown","metadata":{},"source":["Implement from the paper \"Sliced Score Matching: A Scalable Approach to Density and Score Estimation\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sliced_score_matching(model, x, labels,  M, distribution,v=None):\n","    if distribution == \"normal\":\n","        v = torch.randn(M, *x.shape).to(x.device)\n","    elif distribution == \"rademacher\":\n","        v = torch.randint(0, 2, (M, *x.shape)).to(x.device) * 2 - 1\n","    elif distribution == \"pca\" :\n","        v = torch.from_numpy(np.repeat(v[:, np.newaxis, :], x.shape[0], axis=1)) \n","   \n","    v = v.to(x.device).float()\n","    N = x.shape[0]\n","    J = 0 \n","    sm = model(x, labels.to(x.device))\n","    grad_sm = batch_jacobian(input=x, output=sm)\n","    for i in range(N):\n","        for j in range(M): \n","            J += 0.5 * torch.matmul(torch.matmul(v[j][i], grad_sm[i]), v[j][i]) + 0.5 * torch.matmul(v[j][i], sm[i])**2\n","    return J / (N * M)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sliced_score_matching_vr(model, x, labels,  M, distribution):\n","    # print(labels, x.shape, labels.shape)\n","    # M directions\n","    if distribution == \"normal\":\n","        v = torch.randn(M, *x.shape).to(x.device)\n","    elif distribution == \"rademacher\":\n","        v = torch.randint(0, 2, (M, *x.shape)).to(x.device) * 2 - 1\n","    v = v.to(x.device).float()\n","    N = x.shape[0]\n","    J = 0 \n","    sm = model(x, labels.to(x.device))\n","    grad_sm = batch_jacobian(input=x, output=sm)\n","    for i in range(N):\n","        for j in range(M):  \n","            J += 0.5 * torch.matmul(torch.matmul(v[j][i], grad_sm[i]), v[j][i]) + 0.5 * torch.norm(sm[i], p=2)**2\n","    return J / (N * M)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_model(model: object, optimizer: object, path: str) -> None:\n","    torch.save([model.state_dict(), optimizer.state_dict()], path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_sn(\n","    model: object,\n","    train_loader: object,\n","    n_epochs: int,\n","    lr: float,\n","    sigmas: torch.Tensor = torch.Tensor([0.1]),\n","    use_cuda: bool = False,\n","    conditional: bool = True,\n","    loss_type: str = \"denoising_score_matching\",\n","    n_vectors: int = 1,\n","    dist_type: str = \"normal\",\n",") -> dict:\n","    if use_cuda:\n","        critic = model.cuda()\n","    model.train()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0, 0.9))\n","    batch_loss_history = {\"loss\": []}\n","    v = None\n","    if dist_type == \"pca\":\n","        # get the first principal component from the data\n","        pca = PCA(n_components=2) \n","        pca.fit(train_loader.dataset.tensors[0])\n","        v = pca.components_\n","    for epoch_i in tqdm(range(n_epochs)):\n","        mean_loss = 0\n","        for batch_i, x in enumerate(train_loader):\n","            x = x[0]\n","            batch_size = x.shape[0]\n","            if use_cuda:\n","                x = x.cuda()\n","        \n","            if conditional:\n","                labels = torch.randint(len(sigmas), (batch_size,))\n","                sigma_batch = sigmas[labels].to(x.device)\n","                sigma_batch = sigma_batch.reshape(-1, 1)\n","            else:\n","                sigma_batch = (\n","                    sigmas[0] * torch.ones(batch_size, 1, device=x.device).float()\n","                )\n","            if loss_type == \"denoising_score_matching\":\n","                standart_noise = torch.randn_like(x)\n","                x_noisy = x + standart_noise * sigma_batch\n","                optimizer.zero_grad()\n","                if conditional:\n","                    pred_scores = model(x_noisy, labels.to(x.device))\n","                else:\n","                    pred_scores = model(x_noisy)  \n","                noisy_scores = -standart_noise / sigma_batch\n","                losses = torch.sum((pred_scores - noisy_scores) ** 2, axis=-1) / 2\n","                loss = torch.mean(losses * sigma_batch.flatten() ** 2)\n","            elif loss_type == \"sliced_score_matching\":\n","                x.requires_grad_(True)\n","                optimizer.zero_grad()\n","                loss = sliced_score_matching(model, x, labels, n_vectors, dist_type,v)\n","            elif loss_type == \"sliced_score_matching_vr\":\n","                x.requires_grad_(True)\n","                optimizer.zero_grad()\n","                loss = sliced_score_matching_vr(model, x, labels,  n_vectors, dist_type,v)\n","            loss.backward()\n","            optimizer.step()\n","            mean_loss += loss.data.cpu().numpy()\n","        batch_loss_history[\"loss\"].append(mean_loss / len(train_loader))\n","    return model, batch_loss_history"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main():\n","    parser = argparse.ArgumentParser(\n","        description=\"Geneerate samples by estimating the gradient of the data distribution\"\n","    )\n","    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size\")\n","    parser.add_argument(\"--n_epochs\", type=int, default=100, help=\"number of epochs\")\n","    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"learning rate\")\n","    parser.add_argument(\"--lambda_max\", type=float, default=0.01, help=\"lambda max\")\n","    parser.add_argument(\"--lambda_min\", type=float, default=1e-4, help=\"lambda min\")\n","    parser.add_argument(\"--n_lambdas\", type=int, default=10, help=\"number of lambdas\")\n","    parser.add_argument(\"--use_cuda\", type=bool, default=True, help=\"use cuda\")\n","    parser.add_argument(\"--mode\", type=str, default=\"train\", help=\"mode\")\n","    parser.add_argument(\"--n_samples\", type=int, default=5, help=\"number of samples\")\n","    parser.add_argument(\"--n_steps\", type=int, default=100, help=\"number of steps\")\n","    parser.add_argument(\"--save_freq\", type=int, default=50, help=\"save frequency\")\n","    parser.add_argument(\"--model_name\", type=str, default=\"simple\", help=\"model name\")\n","    parser.add_argument(\"--eps\", type=float, default=5e-5, help=\"eps\")\n","    parser.add_argument(\"--dataset\", type=str, default=\"cifar10\")\n","    parser.add_argument(\"--directions\", type=str, default=\"right\")\n","    parser.add_argument(\"--loss_type\", type=str, default=\"denoising\")\n","    parser.add_argument(\"--n_vectors\", type=int, default=1)\n","    parser.add_argument(\"--dist_type\", type=str, default=\"normal\")\n","    parser.add_argument(\"--save\", type=bool, default=True)\n","    args = parser.parse_args()\n","    return args"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == \"__main__\":\n","    torch.cuda.empty_cache()\n","    args = main()\n","    batch_size = args.batch_size\n","\n","    # check cuda\n","    if args.use_cuda:\n","        assert torch.cuda.is_available(), \"CUDA is not available\"\n","    torch.cuda.empty_cache()\n","    # create sigmas\n","    sigmas = torch.tensor(\n","        np.exp(\n","            np.linspace(\n","                np.log(args.lambda_max), np.log(args.lambda_min), args.n_lambdas\n","            )\n","        ),\n","        dtype=torch.float32,\n","    )\n","    print(args.dataset)\n","    if args.dataset == \"mnist\":\n","        train_data, test_data = load_dataset(\n","            args.dataset, flatten=False, binarize=False\n","        )\n","        path = \"./mnist.pth\"\n","    elif args.dataset == \"cifar10\":\n","        train_data, test_data = load_dataset(\n","            args.dataset, flatten=False, binarize=False\n","        )\n","        path = \"./pretrained_models/cifar10.pth\"\n","    elif args.dataset == \"celeba\":\n","        train_data, test_data = load_dataset(\n","            args.dataset, flatten=False, binarize=False\n","        )\n","        path = \"./pretrained_models/celeba.pth\"\n","    elif args.dataset == \"mixture\":\n","        p = 0.2\n","        noise = 0.1\n","        mix = TD.Categorical(torch.tensor([p, 1 - p]).cuda())\n","        mv_normals = TD.MultivariateNormal(\n","            torch.tensor([[1.0, 1.0], [-1.0, -1.0]]).cuda(),\n","            noise * torch.eye(2).unsqueeze(0).cuda(),\n","        )\n","        mixture = TD.MixtureSameFamily(mix, mv_normals)\n","        # check if the dataset is already created\n","        if os.path.exists(\"datasets/train_data.json\"):\n","            with open(\"datasets/train_data.json\", \"r\") as f:\n","                train_data = torch.tensor(json.load(f))\n","            with open(\"datasets/test_data.json\", \"r\") as f:\n","                test_data = torch.tensor(json.load(f))\n","            print(\"Dataset is loaded from json file\")\n","        else:\n","            train_data, test_data = train_test_split(mixture.sample((10000,)))\n","            # save the dataset in a json file\n","            if not os.path.exists(\"datasets\"):\n","                os.makedirs(\"datasets\")\n","            with open(\"datasets/train_data.json\", \"w\") as f:\n","                json.dump(train_data.tolist(), f)\n","            with open(\"datasets/test_data.json\", \"w\") as f:\n","                json.dump(test_data.tolist(), f)\n","            print('Dataset is created and saved in \"datasets\" folder')\n","\n","    # choose model\n","    if args.model_name == \"ncsn\":\n","        model = NoiseConditionalScoreNetwork()\n","    elif args.model_name == \"simple_ncsn\":\n","        model = SimpleNoiseConditionalScoreNetwork(\n","            hidden_dim=512, data_dim=2, num_sigmas=len(sigmas)\n","        )\n","    elif args.model_name == \"condrefinenet\":\n","        model = CondRefineNetDilated()\n","    elif args.model_name == \"simple\":\n","        model = SimpleScoreNetwork(hidden_dim=512)\n","    else:\n","        raise ValueError(\n","            'The argument model_name must have the values \"ncsn\", \"condrefinenet\" or \"refinenet\"'\n","        )\n","\n","    # train or test\n","    if args.mode == \"train\":\n","        logg.info(\"Starting training\")\n","        n_epochs = args.n_epochs\n","        lr = args.lr\n","        train_loader = torch.utils.data.DataLoader(\n","            torch.utils.data.TensorDataset(torch.tensor(train_data)),\n","            batch_size=batch_size,\n","            shuffle=True,\n","        )\n","        # save model in trained_models folder ceate it if not exist\n","        dataset, model_name, loss_type, epochs, samples , n_vectors, dist_type = (\n","            args.dataset,\n","            args.model_name,\n","            args.loss_type,\n","            args.n_epochs,\n","            args.n_samples,\n","            args.n_vectors,\n","            args.dist_type,\n","        )\n","        if not os.path.exists(\"trained_models\"):\n","            os.makedirs(\"trained_models\")\n","        model_path = f\"trained_models/{dataset}_{model_name}_{loss_type}_{epochs}_{dist_type}_{n_vectors}.pth\"\n","        # check if the model is already trained\n","        if os.path.exists(model_path):\n","            model = torch.load(model_path)\n","            print(\"Model is loaded from the saved file\")\n","        else:\n","            model, batch_loss = train_sn(\n","                    model,\n","                    train_loader=train_loader,\n","                    n_epochs=n_epochs,\n","                    sigmas=sigmas,\n","                    lr=args.lr,\n","                    conditional=True,\n","                    use_cuda=args.use_cuda,\n","                    loss_type=args.loss_type,\n","                    n_vectors=args.n_vectors,\n","                    dist_type=args.dist_type,\n","                )\n","            \n","        if args.save == True:\n","            torch.save(\n","                model, model_path\n","            )\n","        \n","        if not os.path.exists(\"training_experiments\"):\n","            os.makedirs(\"training_experiments\")\n","        if loss_type == \"sliced_score_matching\" or loss_type == \"sliced_score_matching_vr\":\n","            exp_folder = (\n","                f\"{dataset}_{model_name}_{loss_type}_epochs_{epochs}_samples_{samples}_n_vectors_{args.n_vectors}_dist_type_{args.dist_type}\"\n","            )\n","        else :\n","            exp_folder = (\n","                f\"{dataset}_{model_name}_{loss_type}_epochs_{epochs}_samples_{samples}\"\n","            )\n","        full_path = os.path.join(\"training_experiments\", exp_folder)\n","        if not os.path.exists(full_path):\n","            os.makedirs(full_path)\n","\n","        # save a config file with parameters of the experiment\n","        \n","        with open(f\"{full_path}/config.txt\", \"w\") as f:\n","            for key, value in args.__dict__.items():\n","                f.write(\"%s:%s\\n\" % (key, value))\n","        # save loss values\n","        with open(f\"{full_path}/loss.txt\", \"w\") as f:\n","            for loss in batch_loss[\"loss\"]:\n","                f.write(\"%s\\n\" % loss)\n","        #save plot of loss after each epoch\n","        plt.plot(batch_loss[\"loss\"])\n","        plt.title(\"Loss\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"Loss\")\n","        plt.savefig(f\"training_experiments/\" + exp_folder + \"/loss.png\")\n","\n","        # save plot of distribution,score\n","        fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n","        plot_score_function(\n","            distribution2score(mixture),\n","            sigmas,\n","            train_data,\n","            \"TwoGaussMixture Score and Samples\",\n","            ax=ax[0],\n","            npts=30,\n","        )\n","  \n","        samples = model.sample(\n","            n_samples=samples, n_steps=args.n_steps, eps=args.eps, sigmas=sigmas\n","        )\n","        plot_score_function(\n","            model,\n","            sigmas,\n","            samples,\n","            \"Predicted scores\",\n","            ax=ax[1],\n","            npts=30,\n","            plot_scatter=False,\n","        )\n","        \n","        plot_score_function(\n","            model, sigmas, samples, \"Predicted scores and samples\", ax=ax[2], npts=30\n","        )\n","        plt.savefig(f\"training_experiments/\" + exp_folder + \"/cond_score.png\")\n","        \n","        # crete an image with only the predicted samples\n","        \n","        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n","        plot_score_function(\n","            model, sigmas, samples, f\"Predicted samples\", ax=ax, npts=30\n","        )\n","        plt.savefig(\"training_experiments/\" + exp_folder + f\"/Predicted_samples {loss_type}_{dist_type}_{n_vectors}.png\")\n","        \n","        # save an image with the actual samples\n","        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n","        plot_score_function( \n","            distribution2score(mixture),\n","            sigmas,\n","            train_data,\n","            \"TwoGaussMixture Score Function\",\n","            ax=ax,\n","            npts=30,\n","        )\n","        plt.savefig(f\"training_experiments/\" + exp_folder + \"/actual_samples.png\")\n","        \n","    elif args.mode == \"test\":\n","        logg.info(\"Starting testing\")\n","        \n","        if args.dataset == \"mixture\":\n","            predicted_losses = test_mix(\n","                mixture= mixture,\n","                test_data=train_data,\n","                sigmas=sigmas,                \n","            )\n","            print(predicted_losses)\n","            labels = os.listdir(\"trained_models\")\n","            for i in range(len(labels)):\n","                labels[i] = \"_\".join(labels[i].split(\"_\")[6:])        \n","            # Create a DataFrame for Seaborn\n","            predicted_losses = [loss.item() for loss in predicted_losses]\n","            df = pd.DataFrame({'Labels': labels, 'Losses': predicted_losses})\n","            sorted_data = sorted(zip(labels, predicted_losses), key=lambda x: x[1])\n","            sorted_labels, sorted_losses = zip(*sorted_data)\n","            # Plot box plot\n","            plt.figure(figsize=(10, 6))\n","            sns.barplot(x=list(sorted_labels), y=list(sorted_losses), palette='viridis')\n","            plt.xlabel('Experiment Labels')\n","            plt.ylabel('Predicted Losses')\n","            plt.title('Histogram of Predicted Losses by Experiment')\n","            plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n","            plt.tight_layout() \n","            plt.savefig(f\"training_experiments/\" + \"/histogram.png\")\n","                    \n","        else :\n","            test_ncsn(\n","                path=path,\n","                sigmas=sigmas,\n","                visualize=True,\n","                use_cuda=args.use_cuda,\n","                n_samples=args.n_samples,\n","                n_steps=args.n_steps,\n","                save_freq=args.save_freq,\n","                eps=args.eps,\n","                dataset=args.dataset,\n","            )\n","    elif args.mode == \"inpaint\":\n","        logg.info(\"Starting inpainting\")\n","        inpaint_loader = torch.utils.data.DataLoader(\n","            torch.utils.data.TensorDataset(torch.tensor(test_data)),\n","            batch_size=batch_size,\n","            shuffle=True,\n","        )\n","        inpaint_ncsn(\n","            path=path,\n","            sigmas=sigmas,\n","            use_cuda=args.use_cuda,\n","            n_samples=args.n_samples,\n","            n_steps=args.n_steps,\n","            dataset=args.dataset,\n","            direction=args.directions,\n","        )\n","    else:\n","        raise ValueError('The argument mode must have the values \"train\" or \"generate\"')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_ncsn(\n","    path: str,\n","    sigmas: torch.Tensor,\n","    visualize: bool = True,\n","    use_cuda: bool = False,\n","    n_samples: int = 5,\n","    n_steps: int = 100,\n","    save_freq: int = 50,\n","    eps: float = 5e-5,\n","    dataset: str = \"mnist\",\n","):\n","    if dataset == \"mnist\":\n","        refine_net = NoiseConditionalScoreNetwork(use_cuda=use_cuda)\n","    elif dataset == \"cifar10\":\n","        print(\"dataset is cifar10\")\n","        refine_net = NoiseConditionalScoreNetwork(\n","            use_cuda=use_cuda, n_channels=3, image_size=32, num_classes=10, ngf=128\n","        )\n","    elif dataset == \"celeba\":\n","        refine_net = NoiseConditionalScoreNetwork(\n","            use_cuda=use_cuda, n_channels=3, image_size=32, num_classes=10, ngf=128\n","        )\n","    print(\"dataset: \", dataset)\n","    print(\"path: \", path)\n","    states = torch.load(path)\n","    pretrained = False\n","    if len(states) == 2:  # optimizer state was also saved in the checkpoint\n","        refine_net.load_state_dict(states[0])\n","        pretrained = True\n","    else:\n","        refine_net.load_state_dict(torch.load(path))\n","    print(\"Model is pretrained: \", pretrained)\n","    refine_net.cuda()\n","    refine_net.eval()\n","    samples, history = refine_net.sample(\n","        n_samples=n_samples, n_steps=n_steps, sigmas=sigmas, eps=eps, save_history=True\n","    )\n","    if visualize:\n","        visualize_history(\n","            samples,\n","            history,\n","            sigmas,\n","            save_freq,\n","            pretrained,\n","            dataset=dataset,\n","            save_folder=f\"{n_samples}_samples_{n_steps}_steps_sigma_{sigmas[0]:.4f}_{sigmas[-1]:.4f}_eps_{eps:.5f}_dataset_{dataset}\",\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_mix(mixture, test_data: torch.Tensor, sigmas: torch.Tensor):\n","    true_scores = distribution2score(mixture)(test_data.cuda(), None)\n","    labels = torch.arange(len(sigmas)).cuda()\n","    labels = labels.repeat_interleave(test_data.size(0) // len(labels))\n","    predicted_losses = []\n","    for model_name in os.listdir(\"trained_models\"):\n","        model = torch.load(f\"trained_models/{model_name}\")\n","        score = model(test_data.cuda(), labels)\n","        loss = 0.5*(torch.norm(score - true_scores, p=2, dim=-1)**2).mean()\n","        predicted_losses.append(loss.detach().cpu().numpy())\n","    return predicted_losses"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def visualize_history(\n","    samples, history, sigmas, save_freq, pretrained, dataset, save_folder=\"samples\"\n","):\n","    print(\"Visualizing history\")\n","    grid_samples = make_grid(samples, nrow=5)\n","    grid_img = grid_samples.permute(1, 2, 0).clip(0, 1)\n","    print(\"Saving images\")\n","    # creae save folder\n","    if not os.path.exists(save_folder):\n","        if pretrained:\n","            save_folder = save_folder + \"_pretrained\"\n","            os.makedirs(save_folder)\n","        else:\n","            os.makedirs(save_folder)\n","    steps_per_sigma = int(len(history) / len(sigmas))\n","    for step in range(len(history)):\n","        sigma_step = step % steps_per_sigma\n","        sigma_idx = step // steps_per_sigma\n","        grid_samples = make_grid(history[step], nrow=5)\n","        grid_img = grid_samples.permute(1, 2, 0).clip(0, 1)\n","        # save images in the save folder after converting them to numpy arrays\n","        grid_img = grid_img.cpu().numpy()\n","        step_size = sigma_step * save_freq\n","        print(\"grid img min max: \", grid_img.min(), grid_img.max())\n","        plt.imsave(\n","            f\"{save_folder}/sigma_{sigmas[sigma_idx]:.4f}_step_{step_size}.png\",\n","            grid_img,\n","        )\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def anneal_Langevin_dynamics_inpainting(\n","    x_mod,\n","    image,\n","    scorenet,\n","    sigmas,\n","    img_size,\n","    n_channels,\n","    direction=\"left\",\n","    n_steps_each=100,\n","    step_lr=0.000008,\n","):\n","    images = []\n","    original_image = image.unsqueeze(1).expand(-1, x_mod.shape[1], -1, -1, -1)\n","    original_image = original_image.contiguous().view(\n","        -1, n_channels, img_size, img_size\n","    )\n","    x_mod = x_mod.view(-1, n_channels, img_size, img_size)\n","    mask = torch.zeros_like(image)\n","    if direction == \"left\":\n","        half_original_image = original_image[:, :, :, : img_size // 2]\n","        mask[:, :, :, : img_size // 2] = 1.0 \n","    elif direction == \"right\":\n","        half_original_image = original_image[:, :, :, img_size // 2 :]\n","        mask[:, :, :, img_size // 2 :] = 1.0\n","    elif direction == \"top\":\n","        half_original_image = original_image[:, :, : img_size // 2, :]\n","        mask[:, :, : img_size // 2, :] = 1.0\n","    elif direction == \"bottom\":\n","        half_original_image = original_image[:, :, img_size // 2 :, :]\n","        mask[:, :, img_size // 2 :, :] = 1.0\n","    occluded_img = image * mask\n","    # save half original image\n","    # save_image(half_original_image, 'inpainting/half_original_image_\n","    with torch.no_grad():\n","        for c, sigma in tqdm(\n","            enumerate(sigmas),\n","            total=len(sigmas),\n","            desc=\"annealed Langevin dynamics sampling\",\n","        ):\n","            labels = torch.ones(x_mod.shape[0], device=x_mod.device) * c\n","            labels = labels.long()\n","            step_size = step_lr * (sigma / sigmas[-1]) ** 2\n","            corrupted_half_image = (\n","                half_original_image + torch.randn_like(half_original_image) * sigma\n","            )\n","            # save corrupted half image\n","            if direction == \"left\":\n","                x_mod[:, :, :, : img_size // 2] = corrupted_half_image\n","            elif direction == \"right\":\n","                x_mod[:, :, :, img_size // 2 :] = corrupted_half_image\n","            elif direction == \"top\":\n","                x_mod[:, :, : img_size // 2, :] = corrupted_half_image\n","            elif direction == \"bottom\":\n","                x_mod[:, :, img_size // 2 :, :] = corrupted_half_image\n","            for s in range(n_steps_each):\n","                images.append(torch.clamp(x_mod, 0.0, 1.0).to(\"cpu\"))\n","                noise = torch.randn_like(x_mod) * np.sqrt(step_size * 2)\n","                grad = scorenet(x_mod, labels)\n","                x_mod = x_mod + step_size * grad + noise\n","                if direction == \"left\":\n","                    x_mod[:, :, :, : img_size // 2] = corrupted_half_image\n","                elif direction == \"right\":\n","                    x_mod[:, :, :, img_size // 2 :] = corrupted_half_image\n","                elif direction == \"top\":\n","                    x_mod[:, :, : img_size // 2, :] = corrupted_half_image\n","                elif direction == \"bottom\":\n","                    x_mod[:, :, img_size // 2 :, :] = corrupted_half_image\n","        #\n","        return images, occluded_img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def inpaint_ncsn(path, sigmas, use_cuda, n_samples, n_steps, dataset, direction):\n","    if dataset == \"mnist\":\n","        refine_net = NoiseConditionalScoreNetwork(\n","            use_cuda=use_cuda, n_channels=1, image_size=28, num_classes=10\n","        )\n","    elif dataset == \"cifar10\":\n","        refine_net = NoiseConditionalScoreNetwork(\n","            use_cuda=use_cuda, n_channels=3, image_size=32, num_classes=10, ngf=128\n","        )\n","    states = torch.load(path)\n","    if len(states) == 2:  # optimizer state was also saved in the checkpoint\n","        refine_net.load_state_dict(states[0])\n","        pretrained = True\n","    else:\n","        refine_net.load_state_dict(torch.load(path))\n","    refine_net.cuda()\n","    refine_net.eval()\n","    # download test samples of MNIST\n","    if dataset == \"mnist\":\n","        train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=False)\n","    if dataset == \"cifar10\":\n","        train_data, test_data = load_dataset(\"cifar10\", flatten=False, binarize=False)\n","    data_loader = torch.utils.data.DataLoader(\n","        test_data,\n","        batch_size=n_samples,\n","        shuffle=False,\n","        num_workers=0,\n","        drop_last=True,\n","    )\n","    test_data = next(iter(data_loader))\n","    samples = torch.rand(\n","        n_samples,\n","        n_samples,\n","        refine_net.n_channels,\n","        refine_net.image_size,\n","        refine_net.image_size,\n","    ).cuda()\n","    # save this test\n","    save_image(\n","        test_data,\n","        \"inpainting/original_{0}_{1}_{2}_{3}.png\".format(\n","            dataset, direction, n_steps, n_samples\n","        ),\n","        nrow=5,\n","    )\n","    images, occluded_img = anneal_Langevin_dynamics_inpainting(\n","        samples,\n","        test_data,\n","        refine_net,\n","        sigmas,\n","        n_steps_each=n_steps,\n","        step_lr=0.00002,\n","        img_size=refine_net.image_size,\n","        n_channels=refine_net.n_channels,\n","        direction=direction,\n","    )\n","    imgs = []\n","    print(\"occluded image shape: \", occluded_img.shape)\n","    print(\"test data shape: \", test_data.shape)\n","    # Convert the occluded image to a grid\n","    occluded_grid = make_grid(\n","        occluded_img,\n","        nrow=1,\n","        normalize=True,\n","        scale_each=True,\n","    )\n","    test_grid = make_grid(\n","        test_data,\n","        nrow=1,\n","        normalize=True,\n","        scale_each=True,\n","    )\n","    print(occluded_grid.shape, test_grid.shape)\n","    for i, sample in tqdm(enumerate(images)):\n","        sample = sample.view(\n","            n_samples**2,\n","            refine_net.n_channels,\n","            refine_net.image_size,\n","            refine_net.image_size,\n","        )\n","        image_grid = make_grid(\n","            sample,\n","            nrow=n_samples,\n","            normalize=True,\n","            scale_each=True,\n","        )\n","        image_grid = torch.cat([occluded_grid, image_grid, test_grid], dim=2)\n","        if i % 10 == 0:\n","            im = Image.fromarray(\n","                image_grid.mul_(255)\n","                .add_(0.5)\n","                .clamp_(0, 255)\n","                .permute(1, 2, 0)\n","                .to(\"cpu\", torch.uint8)\n","                .numpy()\n","            )\n","            imgs.append(im)\n","        # save last image\n","        # save_image(image_grid, 'inpainting/latest_inpainting_{0}_{1}_{2}_{3}_sigma_{4}_{5}_{6}.png'.format(dataset, direction, n_steps, n_samples, sigmas[0], sigmas[-1], i), nrow=n_samples)\n","    imgs[-1].save(\n","        \"inpainting/latest_inpainting_{0}_{1}_{2}_{3}_sigma_{4}_{5}.png\".format(\n","            dataset, direction, n_steps, n_samples, sigmas[0], sigmas[-1]\n","        )\n","    )\n","    imgs[0].save(\n","        \"inpainting/inpainting_{0}_{1}_{2}_{3}_sigma_{4}_{5}.gif\".format(\n","            dataset, direction, n_steps, n_samples, sigmas[0], sigmas[-1]\n","        ),\n","        save_all=True,\n","        append_images=imgs[1:],\n","        optimize=False,\n","        duration=40,\n","        loop=0,\n","    )\n","    # show the gif"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
